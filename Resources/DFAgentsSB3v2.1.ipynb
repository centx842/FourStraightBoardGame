{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Dogfight Agent with StableBaselines-3\n",
    "\n",
    "## **Version  2.1**\n",
    "\n",
    "#### Changes Made:\n",
    "- Comprises of a 1D dogfight environment with comparison of two RL agents; one with DQN and the other with PPO.\n",
    "- Basic Working of Environment is Succesful; Action and Reward Mechanisms similar to Mountain-Car-v0 Open AI Gym Environment.\n",
    "- Plots also added for both RL Models.\n",
    "- Current Environment is extended in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy stable-baselines3 torch shimmy gymnasium gym pandas numpy matplotlib torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import time\n",
    "import sys\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from gymnasium.spaces import Discrete\n",
    "# from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteration Version\n",
    "VERSION = '2.1'\n",
    "\n",
    "#Common Environment Testing Variables\n",
    "GRID_SIZE = 7\n",
    "TARGET = 20.0\n",
    "TESTRUNSTEPS = 100\n",
    "ANIMATIONDELAY = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Implementing the Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        \n",
    "        self.action_space = Discrete(4)         # Define action space: 0 = Move Left, 1 = Move Right, 2 = Move Up, 3 = Move Down        \n",
    "        self.observation_space = Discrete(GRID_SIZE * GRID_SIZE)    # Define observation space: 7x7 Grid Size \n",
    "        \n",
    "        self.current_position = (1,1)               # Initial position of the agent\n",
    "        self.target_position = (5,4)                # Define the target position\n",
    "        \n",
    "        self.total_rewards = 0\n",
    "        self.step_penalty = -0.1\n",
    "        self.boundary_penalty = -1\n",
    "\n",
    "        self.action = None\n",
    "        \n",
    "\n",
    "    def reset(self , seed = None, options = None):\n",
    "        \n",
    "        super().reset(seed=seed, options=options)\n",
    "        \n",
    "        self.current_position = (1,1)               # Reset the agent to the leftmost position\n",
    "        self.total_rewards = 0\n",
    "        \n",
    "        state = self._get_flat_index(self.current_position)\n",
    "        info = {}\n",
    "        \n",
    "        return state, info\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.action = action\n",
    "        x, y = self.current_position\n",
    "        reward = 0.0\n",
    "\n",
    "        \n",
    "        # Update position based on action\n",
    "        if action == 0:  # Move Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Move Right\n",
    "            y = min( GRID_SIZE - 1 , y + 1)\n",
    "        elif action == 2:  # Move Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Move Down\n",
    "            x = min( GRID_SIZE - 1 , x + 1)\n",
    "        \n",
    "        \n",
    "        # Check if the agent reached the target position\n",
    "        self.current_position = (x,y)\n",
    "        if self.current_position == self.target_position:\n",
    "            reward += 1  \n",
    "            self.total_rewards += 1\n",
    "        elif x == 0 or x == GRID_SIZE - 1 or y == 0 or y == GRID_SIZE - 1:\n",
    "            reward = self.boundary_penalty\n",
    "            self.total_rewards += self.boundary_penalty\n",
    "        else:\n",
    "            reward = self.step_penalty  \n",
    "            self.total_rewards += self.step_penalty\n",
    "\n",
    "\n",
    "        #Assessing the Termination Condition\n",
    "        terminated = bool(self.total_rewards >= TARGET)\n",
    "        state = self._get_flat_index(self.current_position)\n",
    "\n",
    "        # Return the current state, reward, done flag, and info (empty)\n",
    "        return state, reward, terminated, False, {}\n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "\n",
    "        # Display the grid and agent's current position\n",
    "        grid = [[\"_\" for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "        grid[self.target_position[0]][self.target_position[1]] = \"T\"  # Target position\n",
    "        grid[self.current_position[0]][self.current_position[1]] = \"A\"  # Agent's position\n",
    "        \n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Add a small delay to make the movement visible\n",
    "        time.sleep(ANIMATIONDELAY/1000) \n",
    "        sys.stdout.flush() \n",
    "\n",
    "    \n",
    "    def _get_flat_index(self, position):\n",
    "        return position[0] * GRID_SIZE + position[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Position initially set to State: 9\n",
      "_ _ _ _ _ _ _\n",
      "_ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (1, 0) \n",
      " _ _ _ _ _ _ _\n",
      "A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 0) \n",
      " A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 0) \n",
      " A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 0) \n",
      " A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (0, 1) \n",
      " _ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 1) \n",
      " _ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 1) \n",
      " _ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (0, 0) \n",
      " A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Up | Agent's Position: (0, 0) \n",
      " A _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (0, 1) \n",
      " _ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      "\n",
      "Episode finished with reward: -20\n"
     ]
    }
   ],
   "source": [
    "# Test the environment\n",
    "env = Environment()\n",
    "\n",
    "# Example usage\n",
    "obs, _ = env.reset()\n",
    "print(f\"Agent's Position initially set to State: {obs + 1}\")\n",
    "env.render()\n",
    "\n",
    "for _ in range(10):\n",
    "    \n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, done, trunc, _ = env.step(action)\n",
    "    env.total_rewards += reward\n",
    "\n",
    "    actionstr = ''\n",
    "    if action == 0:\n",
    "        actionstr = 'Left'\n",
    "    if action == 1:\n",
    "        actionstr = 'Right'\n",
    "    if action == 2:\n",
    "        actionstr = 'Up'\n",
    "    if action == 3:\n",
    "        actionstr = 'Down'\n",
    "\n",
    "\n",
    "    # Creating an output log that can refresh after the specific time dealy with std.out.flush\n",
    "    print(f\"\\r Agent's Action: {actionstr} | Agent's Position: {env.current_position} \\n\", end=' ', flush=True)\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Episode terminated with reward: {env.total_rewards}\")\n",
    "        obs, _ = env.reset()\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"\\nEpisode finished with reward: {env.total_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Implementing the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0: Introducing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DQN hyperparameters\n",
    "DQN_TIMESTEPS = 100000\n",
    "DQN_LEARNING_RATE = 2.5e-3     #2.5e-4\n",
    "BUFFER_SIZE = 1000           #1000000\n",
    "LEARNING_STARTS = 100        #100\n",
    "DQN_BATCH_SIZE = 32          #32\n",
    "TAU = 1.0                    #1.0\n",
    "DQN_GAMMA = 0.99             #0.99\n",
    "EXP_FRACTION = 0.1           #0.1\n",
    "EXP_INITIAL_VAL = 1.0        #1.0\n",
    "EXP_FINAL_VAL = 0.05         #0.05\n",
    "DQN_MAX_GRAD_NORM = 10       #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training complete.\n",
      "Time Elapsed:  2 minutes and 54 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Introducing our DQN Model\n",
    "model = DQN(\"MlpPolicy\", env, \n",
    "            learning_rate=DQN_LEARNING_RATE,\n",
    "            buffer_size=BUFFER_SIZE,\n",
    "            learning_starts=LEARNING_STARTS,\n",
    "            batch_size=DQN_BATCH_SIZE,\n",
    "            tau=TAU,\n",
    "            gamma=DQN_GAMMA,\n",
    "            exploration_fraction=EXP_FRACTION,\n",
    "            exploration_initial_eps=EXP_INITIAL_VAL,\n",
    "            exploration_final_eps=EXP_FINAL_VAL,\n",
    "            max_grad_norm=DQN_MAX_GRAD_NORM,\n",
    "            verbose=0)\n",
    "\n",
    "# Set a Timer here for time elapsed with DQN-learning\n",
    "start_learning_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the Model; Already used override function namely 'step' and 'reset'\n",
    "    trained_model = model.learn(total_timesteps=DQN_TIMESTEPS, \n",
    "                                reset_num_timesteps=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during training: {e}\")\n",
    "    trained_model = None\n",
    "\n",
    "# Stopping the Timer\n",
    "end_learning_time = time.time()\n",
    "\n",
    "# Computing the Time Elapsed\n",
    "if trained_model is not None:\n",
    "    elapsed_learning_time = np.round(end_learning_time - start_learning_time, 2)\n",
    "    print(\"\\n\\nTraining complete.\")\n",
    "    print(\"Time Elapsed:  {} minutes and {} seconds.\".format(int(elapsed_learning_time // 60), int(np.round(elapsed_learning_time % 60))))\n",
    "    # Saving the Model\n",
    "    trained_model.save(f\"DQN Models/DQN_v{VERSION}_trained_{DQN_TIMESTEPS}.zip\")\n",
    "else:\n",
    "    print(\"Training failed. Model not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _\n",
      "_ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (1, 2) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ A _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (1, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (2, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (3, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (3, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (4, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      "Verdict: Target reached!!\n",
      "\n",
      "Episode finished with Total Rewards: 20.4  (completed in 29 steps)\n",
      "End of Simulation\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for t in range(TESTRUNSTEPS):\n",
    "    \n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    env.total_rewards += reward\n",
    "    \n",
    "    actionstr = ''\n",
    "    if action == 0:\n",
    "        actionstr = 'Left'\n",
    "    if action == 1:\n",
    "        actionstr = 'Right'\n",
    "    if action == 2:\n",
    "        actionstr = 'Up'\n",
    "    if action == 3:\n",
    "        actionstr = 'Down'\n",
    "\n",
    "\n",
    "    # Creating an Output Visual    \n",
    "    print(f\"\\r Agent's Action: {actionstr} | Agent's Position: {env.current_position} \\n\", end=' ', flush=True)\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "if (env.total_rewards >= TARGET):\n",
    "    print(f\"Verdict: Target reached!!\")\n",
    "else:\n",
    "    print(f\"Verdict: Target not reached...\")\n",
    "\n",
    "print(f\"\\nEpisode finished with Total Rewards: {env.total_rewards}  (completed in {t} steps)\")\n",
    "print(f\"End of Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Plotting Episode Length and Mean Rewards from Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = \"./dqn_agent_logs/\"\n",
    "# csv_file = log_dir + \"progress.csv\"\n",
    "\n",
    "# #Displaying the Training Progress from the CSV file\n",
    "# data = pd.read_csv(csv_file)\n",
    "# print(data.head(10))\n",
    "\n",
    "\n",
    "# print(f\"\\n\\n\\nPlotting Episode Length and Rewards from Training Logs:\\n\")\n",
    "\n",
    "# #Adjust size of the Plots\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# #Displaying the Plots of Episode Length and Rewards.\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(data['time/time_elapsed'], data['rollout/ep_len_mean'],'r')\n",
    "# plt.title(\"Episode Length Over Time\")\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Mean Episode Length\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(data['time/time_elapsed'], data['rollout/ep_rew_mean'], 'b')\n",
    "# plt.title(\"Rewards Over Time\")\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Mean Episode Reward\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Implementing the PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0: Introducing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PPO hyperparameters\n",
    "PPO_TIMESTEPS = 100000\n",
    "PPO_LEARNING_RATE = 2.5e-3     #2.5e-4\n",
    "BATCH_SIZE = 256             #64\n",
    "N_EPOCHS = 10                #10\n",
    "PPO_GAMMA = 0.9              #0.99\n",
    "GAE_LAMBDA = 0.95            #0.95\n",
    "CLIP_RANGE = 0.2             #0.2\n",
    "ENT_COEF = 0.01\n",
    "VF_COEF = 0.5\n",
    "PPO_MAX_GRAD_NORM = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamza-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 532  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 499         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039083865 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.013      |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033959776 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.878       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04170307 |\n",
      "|    clip_fraction        | 0.482      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.0326     |\n",
      "|    learning_rate        | 0.0025     |\n",
      "|    loss                 | 0.262      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0609    |\n",
      "|    value_loss           | 0.925      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03409542 |\n",
      "|    clip_fraction        | 0.417      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.0696     |\n",
      "|    learning_rate        | 0.0025     |\n",
      "|    loss                 | 0.521      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0548    |\n",
      "|    value_loss           | 1.04       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025483927 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.34        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07106492 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.742     |\n",
      "|    explained_variance   | 0.149      |\n",
      "|    learning_rate        | 0.0025     |\n",
      "|    loss                 | 0.381      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0613    |\n",
      "|    value_loss           | 0.961      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 908        |\n",
      "|    ep_rew_mean          | 20.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04858955 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.543     |\n",
      "|    explained_variance   | 0.204      |\n",
      "|    learning_rate        | 0.0025     |\n",
      "|    loss                 | 0.0906     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    value_loss           | 0.407      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 323         |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008749967 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.197       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    value_loss           | 0.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 213         |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008610738 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.446      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00383    |\n",
      "|    value_loss           | 0.689       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.9         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068145436 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.366        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 0.653        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.6         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011738453 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.437        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 0.000414     |\n",
      "|    value_loss           | 0.656        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033228712 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.319        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 0.646        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.4         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034989016 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.325        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    value_loss           | 0.649        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.4         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035870085 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.268        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 3.69e-05     |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040503787 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.308        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -6.65e-05    |\n",
      "|    value_loss           | 0.653        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024994866 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.236        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.284        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000394    |\n",
      "|    value_loss           | 0.647        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026391963 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.234        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.372        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.000684     |\n",
      "|    value_loss           | 0.66         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030936953 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.355        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00014     |\n",
      "|    value_loss           | 0.655        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033620712 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.469        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0009      |\n",
      "|    value_loss           | 0.659        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012639161 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.233        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.328        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000658    |\n",
      "|    value_loss           | 0.652        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.3        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005915042 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.319       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000569   |\n",
      "|    value_loss           | 0.674       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041297907 |\n",
      "|    clip_fraction        | 0.0657       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.36        |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.332        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 0.662        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026555485 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.258        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 0.649        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028947615 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.337        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000406    |\n",
      "|    value_loss           | 0.641        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022287318 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.342       |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.334        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 0.656        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054810112 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.289        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000829    |\n",
      "|    value_loss           | 0.654        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039405865 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.36         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000728    |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032298258 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.314        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | 0.000184     |\n",
      "|    value_loss           | 0.659        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026617004 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.418        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000865    |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.1        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002991396 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.344       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.000681   |\n",
      "|    value_loss           | 0.665       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014370894 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.265        |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 0.000717     |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 51        |\n",
      "|    ep_rew_mean          | 20.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 481       |\n",
      "|    iterations           | 33        |\n",
      "|    time_elapsed         | 140       |\n",
      "|    total_timesteps      | 67584     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0039368 |\n",
      "|    clip_fraction        | 0.0379    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.313    |\n",
      "|    explained_variance   | 0.228     |\n",
      "|    learning_rate        | 0.0025    |\n",
      "|    loss                 | 0.367     |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.000414 |\n",
      "|    value_loss           | 0.654     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51           |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027254727 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.357        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000465    |\n",
      "|    value_loss           | 0.655        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 148          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014649997 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.368        |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 0.000243     |\n",
      "|    value_loss           | 0.663        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.1        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002739823 |\n",
      "|    clip_fraction        | 0.058       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.279       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 0.65        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030606925 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.283        |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000827    |\n",
      "|    value_loss           | 0.656        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071979854 |\n",
      "|    clip_fraction        | 0.0582       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | 0.233        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.288        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 0.65         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017423925 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | 0.226        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.299        |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000272    |\n",
      "|    value_loss           | 0.654        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.1        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002375713 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.000367   |\n",
      "|    value_loss           | 0.656       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023573348 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.255       |\n",
      "|    explained_variance   | 0.231        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.303        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000213    |\n",
      "|    value_loss           | 0.651        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018821012 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.362        |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000417    |\n",
      "|    value_loss           | 0.652        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 184          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025503873 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | 0.226        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.321        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000585    |\n",
      "|    value_loss           | 0.656        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.3         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 188          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067068473 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.267        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 0.695        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 192          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035148854 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.231        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.307        |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.000196     |\n",
      "|    value_loss           | 0.649        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.1         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 197          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052280994 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.365        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    value_loss           | 0.655        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 201          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028154692 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.255       |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | 0.367        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000802    |\n",
      "|    value_loss           | 0.655        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.2        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002872657 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 0.651       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.4        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003500305 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.326       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000432   |\n",
      "|    value_loss           | 0.653       |\n",
      "-----------------------------------------\n",
      "\n",
      "\n",
      "Training complete.\n",
      "Time Elapsed:  3 minutes and 31 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Introducing our PPO Model\n",
    "model = PPO(\"MlpPolicy\", env, \n",
    "            learning_rate = PPO_LEARNING_RATE,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            n_epochs = N_EPOCHS,\n",
    "            gamma = PPO_GAMMA,\n",
    "            gae_lambda = GAE_LAMBDA,\n",
    "            clip_range = CLIP_RANGE,\n",
    "            ent_coef = ENT_COEF,\n",
    "            vf_coef = VF_COEF,\n",
    "            max_grad_norm = PPO_MAX_GRAD_NORM,\n",
    "            verbose=1)\n",
    "\n",
    "# Setting a new logger for creating a CSV file for training variables\n",
    "# ppo_log_dir = \"./ppo_agent_logs/\"\n",
    "# train_logger = configure(ppo_log_dir, [\"csv\"])\n",
    "# model.set_logger(train_logger)\n",
    "\n",
    "# Set a Timer here for time elapsed with PPO-learning\n",
    "start_learning_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the Model; Already used override function namely 'step' and 'reset'\n",
    "    trained_model = model.learn(total_timesteps=PPO_TIMESTEPS, \n",
    "                                reset_num_timesteps=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during training: {e}\")\n",
    "    trained_model = None\n",
    "\n",
    "# Stopping the Timer\n",
    "end_learning_time = time.time()\n",
    "\n",
    "# Computing the Time Elapsed\n",
    "if trained_model is not None:\n",
    "    \n",
    "    elapsed_learning_time = np.round(end_learning_time - start_learning_time, 2)\n",
    "    print(\"\\n\\nTraining complete.\")\n",
    "    print(\"Time Elapsed:  {} minutes and {} seconds.\".format(int(elapsed_learning_time // 60), int(np.round(elapsed_learning_time % 60))))\n",
    "    \n",
    "    # Saving the Model\n",
    "    trained_model.save(f\"PPO Models/PPO_v{VERSION}_trained_{PPO_TIMESTEPS}.zip\")\n",
    "\n",
    "else:\n",
    "    print(\"Training failed. Model not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _\n",
      "_ A _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (1, 2) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ A _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (1, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (2, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (3, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (3, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (4, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Down | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Right | Agent's Position: (5, 4) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ A _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      " Agent's Action: Left | Agent's Position: (5, 3) \n",
      " _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _\n",
      "_ _ _ A T _ _\n",
      "_ _ _ _ _ _ _\n",
      "\n",
      "\n",
      "Verdict: Target reached!!\n",
      "\n",
      "Episode finished with Total Rewards: 20.4 (completed in 29 steps)\n",
      "End of Simulation\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "for t in range(TESTRUNSTEPS):\n",
    "    \n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    env.total_rewards += reward\n",
    "    \n",
    "    actionstr = ''\n",
    "    if action == 0:\n",
    "        actionstr = 'Left'\n",
    "    if action == 1:\n",
    "        actionstr = 'Right'\n",
    "    if action == 2:\n",
    "        actionstr = 'Up'\n",
    "    if action == 3:\n",
    "        actionstr = 'Down'\n",
    "\n",
    "\n",
    "    # Creating an Output Visual    \n",
    "    print(f\"\\r Agent's Action: {actionstr} | Agent's Position: {env.current_position} \\n\", end=' ', flush=True)\n",
    "    env.render()\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "if (env.total_rewards >= TARGET):\n",
    "    print(f\"Verdict: Target reached!!\")\n",
    "else:\n",
    "    print(f\"Verdict: Target not reached...\")\n",
    "\n",
    "print(f\"\\nEpisode finished with Total Rewards: {env.total_rewards} (completed in {t} steps)\")\n",
    "print(f\"End of Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Plotting Epsiode Length and Mean Rewards from Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time/iterations  time/total_timesteps  time/time_elapsed  time/fps  \\\n",
      "0                1                  2048                  3       622   \n",
      "1                2                  4096                  7       523   \n",
      "2                3                  6144                 12       502   \n",
      "3                4                  8192                 17       478   \n",
      "4                5                 10240                 21       468   \n",
      "\n",
      "   train/entropy_loss  train/explained_variance  train/n_updates  \\\n",
      "0                 NaN                       NaN              NaN   \n",
      "1           -0.677269                  0.006801             10.0   \n",
      "2           -0.611271                  0.069072             20.0   \n",
      "3           -0.501248                  0.073231             30.0   \n",
      "4           -0.395279                  0.173815             40.0   \n",
      "\n",
      "   train/policy_gradient_loss  train/learning_rate  train/approx_kl  \\\n",
      "0                         NaN                  NaN              NaN   \n",
      "1                   -0.065557                0.001         0.017290   \n",
      "2                   -0.066923                0.001         0.026482   \n",
      "3                   -0.060810                0.001         0.044355   \n",
      "4                   -0.037746                0.001         0.117363   \n",
      "\n",
      "   train/value_loss  train/loss  train/clip_fraction  train/clip_range  \n",
      "0               NaN         NaN                  NaN               NaN  \n",
      "1          0.866009    0.231991             0.416162               0.2  \n",
      "2          0.679590    0.244737             0.446289               0.2  \n",
      "3          0.531425    0.145678             0.384766               0.2  \n",
      "4          0.323082    0.057732             0.071240               0.2  \n",
      "\n",
      "\n",
      "\n",
      "Plotting Episode Length and Rewards from Training Logs:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rollout/ep_len_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hamza-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rollout/ep_len_mean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Displaying the Plots of Episode Length and Rewards.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime/time_elapsed\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrollout/ep_len_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode Length Over Time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hamza-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Hamza-pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rollout/ep_len_mean'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log_dir = \"./ppo_agent_logs/\"\n",
    "# csv_file = log_dir + \"progress.csv\"\n",
    "\n",
    "# #Displaying the Training Progress from the CSV file\n",
    "# data = pd.read_csv(csv_file)\n",
    "# print(data.head(10))\n",
    "\n",
    "# print(f\"\\n\\n\\nPlotting Episode Length and Rewards from Training Logs:\\n\")\n",
    "\n",
    "# #Adjust size of the Plots\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# #Displaying the Plots of Episode Length and Rewards.\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(data['time/time_elapsed'], data['rollout/ep_len_mean'],'r')\n",
    "# plt.title(\"Episode Length Over Time\")\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Mean Episode Length\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(data['time/time_elapsed'], data['rollout/ep_rew_mean'], 'b')\n",
    "# plt.title(\"Rewards Over Time\")\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Mean Episode Reward\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
